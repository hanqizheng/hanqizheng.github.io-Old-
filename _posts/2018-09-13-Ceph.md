---
layout:     post
title:      "初识Ceph"
subtitle:   "跟着公司项目，所以来了解一下Ceph"
date:       2018-09-13
author:     "Han Qizheng"
header-img: "img/ceph.jpg"
tags:
    - 分布式存储
---

# 分布式存储
最近用到了Ceph，是一种统一的分布式存储系统。

一上来，Ceph？？分布式存储？？一脸问号。

Google一下才发现分布式存储是一个很大的概念，如果要深入的学那可能需要一段时间，我暂且就先简单的理解一下。

我用一句话简单的概括一下，如果仔细推敲有可能不是很准确，但是对于一个初学者，他可以帮你很好的理解分布式存储
**分布式存储可以简单的理解为，把数据切片分成多段，然后通过某种算法均匀的分别存储在不同的服务器的磁盘上。**

# 什么是Ceph
目前我了解的Ceph是一个开源的，统一的分布式存储系统。

### 统一
统一表示Ceph存储系统可以同时提供对象存储，快存储，文件存储。根据不同的需求满足不同的情况。

### 分布式
分布式表示Ceph是真正的无中心结构和没有理论上限的规模可扩展性。

觉得这些概念很绕口的话，我们就可以简单的理解成Ceph是一个系统，来帮我们实现分布式存储。

# Ceph有3层
## 最底层
最底层也是最核心的一层，也叫做Rados层。由两种节点组成，分别是OSD(Object Storage Device)和monitor

- OSD 是我本次主要关注的一个概念，负责物理存储的daemon进程，可以简单的理解为在硬盘（磁盘）上跑的一个程序，一般情况下是和磁盘一一对应的，一个磁盘跑一个OSD
- monitor，是一个独立部署的daemon进程，提供了整个存储系统的节点信息等全局配置信息

## 中间层
又称 Librados。用于本地或远程通过网络访问rados

## 最上层
对应着ceph不同形式的存储接口实现

> 层的概念暂且先简单的了解一下，不需要深究。

# 一些专业词语
在介绍专业的词语之前，我们要简单的明白几个概念。

**在ceph中，一切皆是对象**。不论是视频，文本，照片等一切格式的数据，Ceph统一将其看作是对象，因为追其根源，所有的数据都是二进制数据保存于磁盘上，所以每一份二进制数据都看成一个对象，不以它们的格式来区分他们。

所以现在我们存储一份数据就可以理解为存储一个对象！

我们最终实现分布式存储的结果就是这个对象被分开存储到不同的若干个Server上。若干个Server被划分为逻辑上的一个集群，其实我们只需要关注每个Server里的磁盘，因为我们现在关心的是存储数据。而刚才我给出了OSD的概念，他是负责管理磁盘的进程，一个磁盘跑一个OSD。

所以，现在就变成存储一个对象到若干OSD

## Pool

Ceph为了保存一个对象，对上构建了一个逻辑层，也就是池(pool)，用于保存对象，这个池的翻译很好的解释了pool的特征，如果把pool比喻成一个中国象棋棋盘，那么保存一个对象的过程就类似于把一粒芝麻放置到棋盘上。

## PG(Placement Group)

Pool再一次进行了细分，即将一个pool划分为若干的PG(归置组 Placement Group)，这类似于棋盘上的方格，所有的方格构成了整个棋盘，也就是说所有的PG构成了一个pool。

## Crush Rule
Crush Rule我现在的理解就是他会设定一些规则，Crush算法会按照这些规则去分布存储数据。

### Crunsh Rule涉及到的故障域有哪些
- OSD级，就是某个OSD坏掉了
- host级，就是某个Server坏了
- rack级，机架损坏。机架上放置了若干台服务器
- pdu级，电源损坏，有可能几个机架共用一个电源
- pod级，机架组，把几个机架逻辑上分为一组，这一组坏掉了
- row级， 若干机架组排列成一行
- room级，整个机房坏掉了
- data center级，一个数据中心有若干机房，整个数据中心炸了....
- region级 北京地区的数据中心，上海的数据中心

# Ceph的基本存储原理(简述)
首先我们要明白，数据最终会分布存储到不同的服务器的不同的磁盘上。

是通过一种映射关系的计算分布下去的

但是如果其中一个磁盘坏掉了，那么这样数据就不完整了。

而且还有一种情况，现在的数据我们一共分布在了10个服务器的磁盘上，但是现在要加磁盘，或者减少磁盘，这样所有的映射关系就要全部改变，非常的不灵活。

所以我们引进了一个中间曾，也就是上述关键词中的PG

有了PG这个中间层，可以说OSD的数量变化对整个分布式存储的映射分布关系影响基本就不存在了。

那么现在整个ceph的存储结构大概就是 数据-->PG-->OSD
## 数据到PG
数据到PG使用了Hash算法，将其根据对象名的hash可以均匀的分布给不同的PG

## PG到OSD
通常，Ceph将一个磁盘看作一个OSD(实际上，OSD是管理一个磁盘的程序)，于是物理层由若干的OSD组成，我们的最终目标是将对象保存到磁盘上，在逻辑层里，对象是保存到PG里面的，那么现在的任务就是打通PG和OSD之间的隧道。PG相当于一堆余数相同的对象的组合，PG把这一部分对象打了个包，现在我们需要把很多的包平均的安放在各个OSD上，这就是CRUSH算法所要做的事情：CRUSH计算PG->OSD的映射关系。


**现在我没有具体关注这些算法是如何将对象平均分布给每个OSD的，所以没有详细叙述**

日后完善一下对象存储，文件存储，快存储的概念。以及RBAC和Key Stone的概念




# 参考
- [大话Ceph--CRUSH那点事儿](http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/)

- [ceph官网](http://docs.ceph.org.cn)